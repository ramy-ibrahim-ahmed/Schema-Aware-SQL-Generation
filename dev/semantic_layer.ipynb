{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d01f33",
   "metadata": {},
   "source": [
    "## Prepare NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5595dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "class GeminiNLP:\n",
    "    def __init__(self, gemini_client: genai):\n",
    "        self.genai = gemini_client\n",
    "\n",
    "    def chat(self, model_name, instructions, messages):\n",
    "        model = self.genai.GenerativeModel(\n",
    "            model_name=model_name, system_instruction=instructions\n",
    "        )\n",
    "        response = model.generate_content(messages)\n",
    "        return response.text\n",
    "\n",
    "    def struct_output(self, model_name, instructions, messages, structure):\n",
    "        model = self.genai.GenerativeModel(\n",
    "            model_name=model_name, system_instruction=instructions\n",
    "        )\n",
    "        response = model.generate_content(\n",
    "            contents=messages,\n",
    "            generation_config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": structure,\n",
    "            },\n",
    "        )\n",
    "        return structure.model_validate_json(response.text)\n",
    "\n",
    "    def func_call(self, model_name, messages, instructions, func):\n",
    "        model = self.genai.GenerativeModel(\n",
    "            model_name=model_name, system_instruction=instructions\n",
    "        )\n",
    "        try:\n",
    "            response = model.generate_content(messages, tools=[func])\n",
    "            call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "            if call:\n",
    "                try:\n",
    "                    result = func(**call.args)\n",
    "                    return result\n",
    "                except Exception as e:\n",
    "                    args_dict = dict(call.args)\n",
    "                    return f\"Error when calling {call.name} with args {args_dict}: {e}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error during model generation: {e}\"\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc761713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_embeddings(vectors: list[list[float]]):\n",
    "    return [\n",
    "        (vec / np.linalg.norm(vec)) if np.linalg.norm(vec) != 0 else vec\n",
    "        for vec in vectors\n",
    "    ]\n",
    "\n",
    "\n",
    "class CohereEmbeddings:\n",
    "    def __init__(self, cohere_client):\n",
    "        self.cohere_client = cohere_client\n",
    "\n",
    "    async def embed(\n",
    "        self,\n",
    "        list_of_text: list[str],\n",
    "        model_name=\"embed-v4.0\",\n",
    "        batch_size=10,\n",
    "    ) -> list[list[float]]:\n",
    "        vectors = []\n",
    "        for i in range(0, len(list_of_text), batch_size):\n",
    "            batch = list_of_text[i : i + batch_size]\n",
    "            response = await self.cohere_client.embed(\n",
    "                texts=batch,\n",
    "                model=model_name,\n",
    "                input_type=\"search_document\",\n",
    "                embedding_types=[\"float\"],\n",
    "                output_dimension=1024,\n",
    "            )\n",
    "            batch_vectors = response.embeddings.float\n",
    "            normalized = normalize_embeddings(batch_vectors)\n",
    "            vectors.extend(normalized)\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a58bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from cohere import AsyncClientV2\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyChTcVwL9R2wWC6GnYeRI1pE4BDaHoIYLU\")\n",
    "nlp = GeminiNLP(gemini_client=genai)\n",
    "\n",
    "cohere_client = AsyncClientV2(api_key=\"q5K7ogyRXviI1pCR4PeTqvvjFyVGLkHvfwlv6EmE\")\n",
    "embedding = CohereEmbeddings(cohere_client=cohere_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24179ea8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604d8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "\n",
    "class ChromaProvider:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.client = None\n",
    "\n",
    "    def connect(self):\n",
    "        self.client = chromadb.PersistentClient(path=self.path)\n",
    "\n",
    "    def create_collection(self, name):\n",
    "        self.client.create_collection(\n",
    "            name=name,\n",
    "            configuration={\"hnsw\": {\"space\": \"cosine\", \"ef_construction\": 200}},\n",
    "        )\n",
    "\n",
    "    def add_points(self, collection_name, ids, embeddings, metadata):\n",
    "        collection = self.client.get_collection(name=collection_name)\n",
    "        collection.add(ids=ids, embeddings=embeddings, metadatas=metadata)\n",
    "\n",
    "    def semantic_search(self, collection_name, vector, top_k):\n",
    "        collection = self.client.get_collection(name=collection_name)\n",
    "        results = collection.query(query_embeddings=vector, n_results=top_k)\n",
    "        return results[\"metadatas\"]\n",
    "\n",
    "    def metadata_filter(self, collection_name, key, value):\n",
    "        collection = self.client.get_collection(name=collection_name)\n",
    "        results = collection.get(where={key: value})\n",
    "        return results[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2679cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "\n",
    "# with open(\"data/sales.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# chunks = data[\"chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91da113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = [f\"id_{i+1}\" for i in range(len(chunks))]\n",
    "# text_chunks = [chunk[\"description\"] for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd5afa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "\n",
    "# embeddings_list = list()\n",
    "# for i in range(0, len(text_chunks), 20):\n",
    "#     batch = text_chunks[i : i + 20]\n",
    "#     batch_embeddings = await embedding.embed(batch)\n",
    "#     embeddings_list.extend(batch_embeddings)\n",
    "#     await asyncio.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef67b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = ChromaProvider(path=\"chromadb\")\n",
    "vectordb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "684fc029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb.create_collection(\"sales\")\n",
    "# vectordb.add_points(\"sales\", ids, embeddings_list, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bea31",
   "metadata": {},
   "source": [
    "## Agentic Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e067113",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09fb1e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_PLAN = \"\"\"Your responsibility is to act as an expert query planner for an ERP system. Your goal is to deconstruct a user's question into a set of strategic search queries. These queries will be used to search a vector store containing column metadata (table_name, column_name, column_description).\n",
    "\n",
    "**Crucial Context:** The `column_description` field explains the **business purpose** of each column (e.g., \"Unique identifier for a sales order,\" \"Net amount for an invoice line,\" \"Customer master record key\"). Your search queries must target this *business meaning*, not just the literal words in the user's question.\n",
    "\n",
    "**Your Process:**\n",
    "1.  **Analyze the User's Question:** Break it down into its core components:\n",
    "    * **Metrics:** What is being measured? (e.g., total amount, count of items, average price).\n",
    "    * **Dimensions/Filters:** What is the data being grouped or filtered by? (e.g., by customer, in a specific date range, for a product category, by region).\n",
    "    * **Relationships:** What business concepts need to be linked? (e.g., \"sales *for a customer*\" implies joining `sales` and `customer` tables).\n",
    "\n",
    "2.  **Generate Strategic Queries:** Based on your analysis, create a list of queries designed to find the specific columns needed to build the final SQL.\n",
    "    * **For Metrics:** Generate queries for the *business concept* of the metric.\n",
    "        * *Example:* If the user asks for \"total revenue,\" search for `\"column for total sales amount\"` or `\"metric for net revenue\"`.\n",
    "    * **For Dimensions/Filters:** Generate queries for the *business purpose* of each filter.\n",
    "        * *Example:* If the user filters by \"customer name,\" search for `\"column for customer name\"` or `\"customer master name field\"`.\n",
    "        * *Example:* If the user filters by \"last month,\" search for `\"column for order date\"` or `\"transaction timestamp\"`.\n",
    "    * **For Joins (Foreign Keys):** When you identify a relationship, generate queries to find the keys that link the tables.\n",
    "        * *Example:* To link sales and customers, search for `\"foreign key linking sales to customers\"` or `\"customer ID in sales table\"` and `\"primary key for customer table\"`.\n",
    "\n",
    "**Instructions:**\n",
    "* Think step-by-step to identify all necessary pieces of information.\n",
    "* Generate multiple, specific queries. It's better to have more targeted queries than one vague one.\n",
    "* Focus on *business terminology* relevant to an ERP (e.g., \"general ledger,\" \"invoice line,\" \"bill of materials,\" \"customer master,\" \"sales order header\").\n",
    "\n",
    "user question: {user_message}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_SQL = \"\"\"You atr a SQL agent, You will take user question with relevant columns names with its description and its table name.\n",
    "You need to use wanted columns only to generate the SQL statement. Make sure that your query follows **Oracle 11g**.\n",
    "\n",
    "user question: {user_message}\n",
    "\n",
    "candidate relevant database schema:\n",
    "\n",
    "{schema}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ce89a",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666c249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: list[str] = Field(\n",
    "        ..., description=\"Search queries to get relevant column names\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SQL(BaseModel):\n",
    "    sql: str = Field(..., description=\"Correct Oracle 11g SQL statement\")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    user_message: str\n",
    "    queries: Queries\n",
    "    schema: str\n",
    "    sql_query: SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bce20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner(state: State) -> State:\n",
    "    user_message = state.get(\"user_message\", \"\")\n",
    "    prompt = PROMPT_PLAN.format(user_message=user_message)\n",
    "    response = nlp.struct_output(\n",
    "        \"gemini-2.5-flash\",\n",
    "        \"you are an sql generation planner agent\",\n",
    "        prompt,\n",
    "        Queries,\n",
    "    )\n",
    "    return {\"queries\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77a3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search(state: State) -> State:\n",
    "    queries = state.get(\"queries\").queries\n",
    "    vectors = await embedding.embed(queries)\n",
    "\n",
    "    results = vectordb.semantic_search(\"sales\", vectors, 10)\n",
    "    flatten = [item for sublist in results for item in sublist]\n",
    "\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for res in flatten:\n",
    "        key = res[\"column_name\"]\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(res)\n",
    "\n",
    "    schema_list = [\n",
    "        f\"Table: {res['table_name']}\\nColumn: {res['column_name']}\\nDescription: {res['description']}\"\n",
    "        for res in unique\n",
    "    ]\n",
    "    schema_text = \"\\n\\n---\\n\\n\".join(schema_list)\n",
    "\n",
    "    return {\"schema\": schema_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f310ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql(state: State) -> State:\n",
    "    user_message = state.get(\"user_message\")\n",
    "    schema = state.get(\"schema\")\n",
    "    prompt = PROMPT_SQL.format(user_message=user_message, schema=schema)\n",
    "    response = nlp.struct_output(\n",
    "        \"gemini-2.5-flash\",\n",
    "        \"you are an Oracle 11g sql generation agent\",\n",
    "        prompt,\n",
    "        SQL,\n",
    "    )\n",
    "    return {\"sql_query\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "621a2537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"plan\", planner)\n",
    "workflow.add_node(\"search\", search)\n",
    "workflow.add_node(\"sql\", sql)\n",
    "\n",
    "workflow.set_entry_point(\"plan\")\n",
    "workflow.add_edge(\"plan\", \"search\")\n",
    "workflow.add_edge(\"search\", \"sql\")\n",
    "workflow.add_edge(\"sql\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7812c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': {'queries': Queries(queries=['column for net profit amount', 'column for gross profit amount', 'column for sales order date', 'column for transaction date', 'column for revenue amount', 'column for cost of goods sold'])}}\n",
      "{'search': {'schema': 'Table: PURCHS_BILL_MST_AI_VW\\nColumn: PYMNT_CSH\\nDescription: Cash payment amount for the purchase invoice.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: CSH_AMT\\nDescription: Total cash payment amount received for the sales bill.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: TAX_AMT\\nDescription: Total tax amount calculated for the sales transaction.\\n\\n---\\n\\nTable: GLS_PST_AI_VW\\nColumn: DR_AMT_F\\nDescription: Debit amount in foreign currency.\\n\\n---\\n\\nTable: GLS_PST_AI_VW\\nColumn: DR_AMT_L\\nDescription: Debit amount in local currency.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: BNK_AMT\\nDescription: Total amount received through bank transfer or cheque payments.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: DSCNT_AMT\\nDescription: Total discount amount applied to the sales bill.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: CPN_AMT\\nDescription: Total coupon amount applied to the sales bill.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: CRDT_AMT\\nDescription: Total amount recorded as credit sales in this bill.\\n\\n---\\n\\nTable: PURCHS_BILL_MST_AI_VW\\nColumn: TOTL_BILL_AMT\\nDescription: Total purchase bill amount in foreign currency.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: CRT_DATE\\nDescription: Date and time when the sales bill record was created in the system.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: MNL_DATE\\nDescription: Manual entry date if the sales bill date is entered manually instead of system-generated.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: DOC_DATE\\nDescription: Official date of the sales bill creation, representing the transaction date.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: YR_NO\\nDescription: Fiscal year number associated with the sales bill.\\n\\n---\\n\\nTable: GLS_PST_AI_VW\\nColumn: SMAN_CODE_NM\\nDescription: Salesman name or description related to the sales entry.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: MNL_NO\\nDescription: Manual document number if the sales bill was manually issued.\\n\\n---\\n\\nTable: INV_V_ITM_MOVMNT_AI\\nColumn: DOCUMENT_DATE\\nDescription: Date of the document/transaction, critical for ERP time-based reporting, period closing, and historical trend analysis.\\n\\n---\\n\\nTable: GLS_PST_AI_VW\\nColumn: GL_DATE\\nDescription: General ledger posting date used for accounting period control.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: DOC_DSC\\nDescription: Description or remarks field for the sales bill transaction.\\n\\n---\\n\\nTable: GLS_PST_AI_VW\\nColumn: CUR_CODE\\nDescription: Currency code used for the transaction (e.g., USD, EUR).\\n\\n---\\n\\nTable: GLS_PST_AI_VW\\nColumn: AC_TYP\\nDescription: Specifies the account type such as asset, liability, expense, or revenue.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: AC_AMT\\nDescription: Total amount recorded under account payments.\\n\\n---\\n\\nTable: SALES_BILL_DTL_AI_VW\\nColumn: ITM_STK_CST\\nDescription: Stock cost of the item, representing its valuation in inventory or cost of goods sold (COGS).\\n\\n---\\n\\nTable: SALES_BILL_DTL_AI_VW\\nColumn: ITM_TOTL_STK_CST\\nDescription: Total stock cost for the sold quantity of the item, used for cost analysis and inventory tracking.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: AC_CODE\\nDescription: General ledger account code representing the main account for the sales transaction.\\n\\n---\\n\\nTable: SALES_BILL_MST_AI_VW\\nColumn: STK_RATE\\nDescription: Stock valuation rate used for cost or accounting calculations in the sales bill.'}}\n",
      "{'sql': {'sql_query': SQL(sql=\"SELECT SUM((NVL(mst.CSH_AMT, 0) + NVL(mst.BNK_AMT, 0) + NVL(mst.CRDT_AMT, 0) + NVL(mst.AC_AMT, 0) - NVL(mst.DSCNT_AMT, 0) - NVL(mst.CPN_AMT, 0)) - NVL(cogs.total_cogs_per_bill, 0)) AS profit_last_month FROM SALES_BILL_MST_AI_VW mst LEFT JOIN (SELECT BILL_ID, SUM(NVL(ITM_TOTL_STK_CST, 0)) AS total_cogs_per_bill FROM SALES_BILL_DTL_AI_VW GROUP BY BILL_ID) cogs ON mst.BILL_ID = cogs.BILL_ID WHERE mst.DOC_DATE >= TRUNC(ADD_MONTHS(SYSDATE, -1), 'MM') AND mst.DOC_DATE < TRUNC(SYSDATE, 'MM')\")}}\n"
     ]
    }
   ],
   "source": [
    "async for event in graph.astream({\"user_message\": \"عايز اعرف الربح اخر شهر\"}):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd6eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(r\"./data/sql_chats\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "provider = \"HuggingFaceTB/\"\n",
    "model_version = \"SmolLM2-360M-Instruct\"\n",
    "model_name = provider + model_version\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    use_cache=False,\n",
    "    cache_dir=\"./models/\" + model_version,\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    cache_dir=\"./tokenizers/\" + model_version,\n",
    ")\n",
    "\n",
    "finetune_name = f\"./results/{model_version}\"\n",
    "finetune_tags = [\"smol-sql\", \"v1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "# from peft import LoraConfig\n",
    "\n",
    "# rank_dimension = 6\n",
    "# lora_alpha = 8\n",
    "# lora_dropout = 0.05\n",
    "\n",
    "# peft_config = LoraConfig(\n",
    "#     r=rank_dimension,\n",
    "#     lora_alpha=lora_alpha,\n",
    "#     lora_dropout=lora_dropout,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=\"all-linear\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=finetune_name,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=6,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    packing=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import AutoPeftModelForCausalLM\n",
    "# from transformers import pipeline\n",
    "\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     pretrained_model_name_or_path=args.output_dir,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )\n",
    "\n",
    "# merged_model = model.merge_and_unload()\n",
    "# merged_model.save_pretrained(\n",
    "#     args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    "# )\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    "# )\n",
    "\n",
    "# del model\n",
    "# del trainer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=finetune_name,\n",
    "    use_cache=False,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=finetune_name)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [],
   "source": [
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "system_prompt = \"You are a SQL assistant\"\n",
    "sql_prompt = \"What is the total volume of timber sold by each salesperson, sorted by salesperson?\"\n",
    "sql_context = \"CREATE TABLE salesperson (salesperson_id INT, name TEXT, region TEXT); INSERT INTO salesperson (salesperson_id, name, region) VALUES (1, 'John Doe', 'North'), (2, 'Jane Smith', 'South'); CREATE TABLE timber_sales (sales_id INT, salesperson_id INT, volume REAL, sale_date DATE); INSERT INTO timber_sales (sales_id, salesperson_id, volume, sale_date) VALUES (1, 1, 120, '2021-01-01'), (2, 1, 150, '2021-02-01'), (3, 2, 180, '2021-01-01');\"\n",
    "user = f\"{sql_prompt}\\n\\nDatabase schema:\\n{sql_context}\"\n",
    "\n",
    "prompt = [\n",
    "    {\"content\": system_prompt, \"role\": \"system\"},\n",
    "    {\"content\": user, \"role\": \"user\"},\n",
    "]\n",
    "\n",
    "print(test_inference(prompt))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
